import os
import time
import csv
# Set dummy video driver to run headless
os.environ["SDL_VIDEODRIVER"] = "dummy"

import pygame
from maze_simulation import MazeSimulation, GenerationAlgorithm, SolvingAlgorithm, CellType

class HeadlessSimulation(MazeSimulation):
    """
    Subclass of MazeSimulation to run without the main loop and rendering.
    """
    def __init__(self):
        # Initialize pygame with dummy driver
        super().__init__()
        
    def run_benchmark(self, grid_size, num_iterations=5):
        """
        Run benchmark for all algorithms on a specific grid size.
        """
        results = []
        
        # Override grid size
        self.grid_size = grid_size
        self.cell_size = max(3, (self.window_width - self.sidebar_width) // self.grid_size)
        self.grid_rows = self.window_height // self.cell_size
        self.end_pos = (self.grid_rows - 2, self.grid_size - 2)
        
        print(f"Benchmarking Grid Size: {self.grid_size}x{self.grid_rows}")
        
        # Test Generation Algorithms
        for gen_algo in GenerationAlgorithm:
            print(f"  Testing {gen_algo.value}...")
            total_time = 0
            total_steps = 0
            total_memory = 0
            
            for _ in range(num_iterations):
                self.generation_algorithm = gen_algo
                self.start_generation()
                
                # Run until complete
                while self.is_generating:
                    if self.generation_algorithm == GenerationAlgorithm.RECURSIVE_BACKTRACKING:
                        self.step_recursive_backtracking()
                    elif self.generation_algorithm == GenerationAlgorithm.PRIMS:
                        self.step_prims_algorithm()
                    elif self.generation_algorithm == GenerationAlgorithm.KRUSKALS:
                        self.step_kruskals_algorithm()
                
                metrics = self.analyzer.stop_tracking()
                total_time += metrics['time_seconds']
                total_steps += metrics['steps']
                total_memory += metrics['peak_memory_mb']
            
            avg_time = total_time / num_iterations
            avg_steps = total_steps / num_iterations
            avg_memory = total_memory / num_iterations
            
            results.append({
                'Type': 'Generation',
                'Algorithm': gen_algo.value,
                'Grid Size': f"{self.grid_size}x{self.grid_rows}",
                'Avg Time (s)': f"{avg_time:.4f}",
                'Avg Steps': f"{avg_steps:.1f}",
                'Avg Memory (MB)': f"{avg_memory:.2f}",
                'Optimality': 'N/A'
            })

        # Test Solving Algorithms (using a fixed maze generated by Recursive Backtracking)
        print("  Testing Solving Algorithms...")
        self.generation_algorithm = GenerationAlgorithm.RECURSIVE_BACKTRACKING
        self.start_generation()
        while self.is_generating:
            self.step_recursive_backtracking()
        self.analyzer.stop_tracking() # Stop tracking for generation
            
        # Get optimal path length using BFS first
        self.solving_algorithm = SolvingAlgorithm.BFS
        self.start_solving()
        while self.is_solving:
            self.step_bfs()
        bfs_metrics = self.analyzer.stop_tracking()
        optimal_len = len(self.solution_path)
        
        for solve_algo in SolvingAlgorithm:
            print(f"    Testing {solve_algo.value}...")
            total_time = 0
            total_steps = 0
            total_memory = 0
            total_path_len = 0
            
            for _ in range(num_iterations):
                self.solving_algorithm = solve_algo
                self.start_solving()
                
                while self.is_solving:
                    if self.solving_algorithm == SolvingAlgorithm.DFS:
                        self.step_dfs()
                    elif self.solving_algorithm == SolvingAlgorithm.ASTAR:
                        self.step_astar()
                    elif self.solving_algorithm == SolvingAlgorithm.BFS:
                        self.step_bfs()
                
                metrics = self.analyzer.stop_tracking()
                total_time += metrics['time_seconds']
                total_steps += metrics['steps']
                total_memory += metrics['peak_memory_mb']
                total_path_len += len(self.solution_path)
            
            avg_time = total_time / num_iterations
            avg_steps = total_steps / num_iterations
            avg_memory = total_memory / num_iterations
            avg_path_len = total_path_len / num_iterations
            optimality = avg_path_len / optimal_len if optimal_len > 0 else 0
            
            results.append({
                'Type': 'Solving',
                'Algorithm': solve_algo.value,
                'Grid Size': f"{self.grid_size}x{self.grid_rows}",
                'Avg Time (s)': f"{avg_time:.4f}",
                'Avg Steps': f"{avg_steps:.1f}",
                'Avg Memory (MB)': f"{avg_memory:.2f}",
                'Optimality': f"{optimality:.2f}"
            })
            
        return results

def main():
    sim = HeadlessSimulation()
    all_results = []
    
    # Test different grid sizes
    grid_sizes = [20, 40, 60]
    
    for size in grid_sizes:
        all_results.extend(sim.run_benchmark(size, num_iterations=3))
        
    # Output results
    print("\nBenchmark Results:")
    print("-" * 100)
    print(f"{'Type':<12} {'Algorithm':<25} {'Grid Size':<10} {'Time (s)':<10} {'Steps':<10} {'Mem (MB)':<10} {'Optimality':<10}")
    print("-" * 100)
    
    for r in all_results:
        print(f"{r['Type']:<12} {r['Algorithm']:<25} {r['Grid Size']:<10} {r['Avg Time (s)']:<10} {r['Avg Steps']:<10} {r['Avg Memory (MB)']:<10} {r['Optimality']:<10}")
        
    # Save to CSV
    with open('benchmark_results.csv', 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=all_results[0].keys())
        writer.writeheader()
        writer.writerows(all_results)
    
    print("\nResults saved to benchmark_results.csv")
    
    # Generate HTML Report
    generate_html_report(all_results)
    print("Report saved to benchmark_report.html")

def generate_html_report(results):
    """Generate an interactive HTML report with charts."""
    import json
    
    # Prepare data for charts
    # We want to group by Grid Size and compare Algorithms
    
    # Separate Generation and Solving results
    gen_results = [r for r in results if r['Type'] == 'Generation']
    solve_results = [r for r in results if r['Type'] == 'Solving']
    
    # Get unique grid sizes and algorithms
    grid_sizes = sorted(list(set(r['Grid Size'] for r in results)), key=lambda x: int(x.split('x')[0]))
    gen_algos = sorted(list(set(r['Algorithm'] for r in gen_results)))
    solve_algos = sorted(list(set(r['Algorithm'] for r in solve_results)))
    
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Maze Simulation Benchmark Results</title>
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        <style>
            body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background: #f0f2f5; }}
            .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 12px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
            h1 {{ color: #1a1a1a; border-bottom: 2px solid #eee; padding-bottom: 10px; }}
            h2 {{ color: #444; margin-top: 40px; }}
            .chart-container {{ position: relative; height: 300px; width: 100%; margin-bottom: 30px; }}
            .row {{ display: flex; flex-wrap: wrap; gap: 20px; }}
            .col {{ flex: 1; min-width: 300px; background: #fff; border: 1px solid #eee; border-radius: 8px; padding: 15px; }}
            table {{ width: 100%; border-collapse: collapse; margin-top: 20px; }}
            th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
            th {{ background-color: #f8f9fa; font-weight: 600; }}
            tr:hover {{ background-color: #f5f5f5; }}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>Maze Simulation Performance Analysis</h1>
            
            <h2>Generation Algorithms Performance</h2>
            <div class="row">
                <div class="col">
                    <canvas id="genTimeChart"></canvas>
                </div>
                <div class="col">
                    <canvas id="genStepsChart"></canvas>
                </div>
            </div>
            
            <h2>Solving Algorithms Performance</h2>
            <div class="row">
                <div class="col">
                    <canvas id="solveTimeChart"></canvas>
                </div>
                <div class="col">
                    <canvas id="solveStepsChart"></canvas>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <canvas id="solvePathChart"></canvas>
                </div>
                <div class="col">
                    <canvas id="solveOptimalityChart"></canvas>
                </div>
            </div>

            <h2>Raw Data</h2>
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Algorithm</th>
                        <th>Grid Size</th>
                        <th>Time (s)</th>
                        <th>Steps</th>
                        <th>Memory (MB)</th>
                        <th>Optimality</th>
                    </tr>
                </thead>
                <tbody>
                    {''.join(f"<tr><td>{r['Type']}</td><td>{r['Algorithm']}</td><td>{r['Grid Size']}</td><td>{r['Avg Time (s)']}</td><td>{r['Avg Steps']}</td><td>{r['Avg Memory (MB)']}</td><td>{r['Optimality']}</td></tr>" for r in results)}
                </tbody>
            </table>
        </div>

        <script>
            const gridSizes = {json.dumps(grid_sizes)};
            
            function createChart(ctxId, label, data, algos, metricKey) {{
                const ctx = document.getElementById(ctxId).getContext('2d');
                const datasets = algos.map((algo, index) => ({{
                    label: algo,
                    data: gridSizes.map(size => {{
                        const item = data.find(r => r['Algorithm'] === algo && r['Grid Size'] === size);
                        return item ? parseFloat(item[metricKey]) : 0;
                    }}),
                    backgroundColor: `hsl(${{index * 360 / algos.length}}, 70%, 60%)`,
                    borderColor: `hsl(${{index * 360 / algos.length}}, 70%, 50%)`,
                    borderWidth: 1
                }}));

                new Chart(ctx, {{
                    type: 'bar',
                    data: {{ labels: gridSizes, datasets: datasets }},
                    options: {{
                        responsive: true,
                        maintainAspectRatio: false,
                        plugins: {{
                            title: {{ display: true, text: label }},
                            legend: {{ position: 'bottom' }}
                        }},
                        scales: {{
                            y: {{ beginAtZero: true }}
                        }}
                    }}
                }});
            }}

            const genResults = {json.dumps(gen_results)};
            const solveResults = {json.dumps(solve_results)};
            const genAlgos = {json.dumps(gen_algos)};
            const solveAlgos = {json.dumps(solve_algos)};

            createChart('genTimeChart', 'Generation Time (s)', genResults, genAlgos, 'Avg Time (s)');
            createChart('genStepsChart', 'Generation Steps', genResults, genAlgos, 'Avg Steps');
            
            createChart('solveTimeChart', 'Solving Time (s)', solveResults, solveAlgos, 'Avg Time (s)');
            createChart('solveStepsChart', 'Solving Steps', solveResults, solveAlgos, 'Avg Steps');
            createChart('solvePathChart', 'Memory Usage (MB)', solveResults, solveAlgos, 'Avg Memory (MB)');
            createChart('solveOptimalityChart', 'Optimality Ratio (Lower is Better)', solveResults, solveAlgos, 'Optimality');
        </script>
    </body>
    </html>
    """
    
    with open('benchmark_report.html', 'w', encoding='utf-8') as f:
        f.write(html_content)

if __name__ == "__main__":
    main()
